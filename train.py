# -*- coding: utf-8 -*-
"""Pothole Detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1krBM0go8GCMWPhUJzTjZNVcndTKrmHv8

To fetch data directly from drive it needs to be mounted
"""

import keras
import tensorflow as tf
from keras.layers import Input,Lambda, Dense, Flatten
from keras.applications import vgg16 # vgg16 is a pretrained model which contains with accuracy of 92.7 % using ImageNet which is a dataset of over 14 million images belonging to 1000 classes

# for pothole detection project purpose dateset has been uploaded in drive 
# to read the images drive needs to be first mounted 

from google.colab import drive
drive.mount('/content/drive')

# setting train path and validate path

train_path = '/content/drive/My Drive/Potholes Dataset/Train'
validate_path = '/content/drive/My Drive/Potholes Dataset/Validate'
print(train_path)
print(validate_path)

# training the base model
# VGG16 model is trained on RGB images of size (224, 224), which is a default input size of the network.  3 represents the number of color channels (R,G,B)
# include_top states whether base classifier to be used or not
# weights refers to the pretrained weights for which argument is to be set as 'imagenet'. if building the model from scracth then weights need to be set as None
# pooling can be of 2 types max or avg. Pooling does the job of reducing the number of parameters and computation power. In most cases value used is 'max'.

vg_model = vgg16.VGG16(input_shape=(224,224,3),include_top=False,weights = 'imagenet',pooling='max')
vg_model.summary()

# as all the layers have already been trained and features are already extracted, no need to re train the layers
# decision to re train the layers totally depend upon number of parameters and records in the dataset
# if datasize is too small i.e. less than 1000 and images class is not part of VGG dataset then its better to re train last few layers to extract features specific to the dataset

channel_name = 'training'

def train():

    for layers in vg_model.layers:
      layers.trainable = False

    # to get the number of classes (folders) inside train dataset
    from glob import glob
    folder = glob('/content/drive/My Drive/Potholes Dataset/Train/*')
    folder

    # flatten is used to convert convolutional layer 4D output to 2D output which is accepted by Dense layer
    # Dense layer is a classifier which does the job of reducing 512 classes to 2 

    flaten_ouput = Flatten()(vg_model.output)
    prediction = Dense(len(folder),activation='sigmoid')(flaten_ouput)

    # optimizing the model

    from keras.models import Model
    model = Model(inputs = vg_model.input,outputs = prediction)
    model.summary()

    # compiling the model
    model.compile(loss='binary_crossentropy',optimizer= 'rmsprop',metrics='accuracy')

    # to fit the model images needs to be in same size for which imagegenerate needs to be applied

    from keras.preprocessing.image import ImageDataGenerator

    gen_train_data = ImageDataGenerator(rescale = 1. / 255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)

    gen_validate_data = ImageDataGenerator(rescale = 1. /255)

    train_generator = gen_train_data.flow_from_directory(train_path,target_size=(224,224),batch_size=16,class_mode= 'categorical')
    validate_generator = gen_validate_data.flow_from_directory(validate_path,target_size=(224,224),batch_size=16,class_mode= 'categorical')

    final_model = model.fit(train_generator,epochs=10,validation_data= validate_generator,steps_per_epoch=len(train_generator),validation_steps=len(validate_generator))

if __name__ == '__main__':
    train()